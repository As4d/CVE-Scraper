from bs4 import BeautifulSoup
import requests, re, pprint, json
import xlsxwriter

CveIds = []
PublishDates = []
UpdateDates = []
CVSSscore = []
confidentialityImpact = []
integrityImpact = []
availibilityImpact = []
accessComplexity = []
authentication = []
gainedAccess = []
vulnType = []


def writeToExcel(filename):
	workbook = xlsxwriter.Workbook(filename + ".xlsx")
	worksheet = workbook.add_worksheet()
	worksheet.write('A1', "CveIds")
	worksheet.write('B1', "PublishDates")
	worksheet.write('C1', "UpdateDates")
	worksheet.write('D1', "CVSSscore")
	worksheet.write('E1', "confidentialityImpact")
	worksheet.write('F1', "integrityImpact")
	worksheet.write('G1', "availibilityImpact")
	worksheet.write('H1', "authentication")
	worksheet.write('I1', "gainedAccess")
	worksheet.write('J1', "vulnType")

	for data in range(2, len(CveIds)):
		worksheet.write('A' + str(data), CveIds[data])
		worksheet.write('B' + str(data), PublishDates[data])
		worksheet.write('C' + str(data), UpdateDates[data])
		worksheet.write('D' + str(data), CVSSscore[data])
		worksheet.write('E' + str(data), confidentialityImpact[data])
		worksheet.write('F' + str(data), integrityImpact[data])
		worksheet.write('G' + str(data), availibilityImpact[data])
		worksheet.write('H' + str(data), authentication[data])
		worksheet.write('I' + str(data), gainedAccess[data])
		worksheet.write('J' + str(data), vulnType[data])
	workbook.close()


def getSoupHTML(url):
	response = requests.get(url)
	html = response.content
	soup = BeautifulSoup(html, "html.parser")
	return soup


def getCVEIds(soup, cveArray):
	table = soup.find("table", attrs={"class", "searchresults"})
	for a in table.find_all("a", href=True):
		m = re.search("CVE-\d{4}-\d{4,7}", a["href"])
		if m:
			cveArray.append(m.group(0))


def getPublishDates(arr):
	for date in arr:
		PublishDates.append(date[0])


def getUpdateDates(arr):
	for date in arr:
		UpdateDates.append(date[1])


def getDates(soup, dates):
	table = soup.find("table", attrs={"class", "searchresults"})
	for td in table.find_all("td"):
		m = re.search("\d{4}-\d{2}-\d{2}", str(td))
		if m:
			dates.append(m.group(0))
	it = iter(dates)
	arr = list(zip(it, it))
	getUpdateDates(arr)
	getPublishDates(arr)


def getCVEPages(soup):
	cveIDPages = []
	items = soup.find_all("div", class_="paging")
	for item in items:
		links = item.find_all("a")
		for link in links:
			cveIDPages.append("http://www.cvedetails.com/" + str(link["href"]))

	return cveIDPages


def getCVEDetails(cveid=""):
	cveUrl = "http://www.cvedetails.com/cve/{}/".format(cveid)
	response = requests.get(cveUrl)
	cveHtml = response.content
	soup = BeautifulSoup(cveHtml, "html.parser")
	if soup == "":
		return
	CveIds.append(cveid)
	cvssTable = soup.find(id="cvssscorestable")
	cvssData = []
	for row in cvssTable.findAll("tr"):
		cols = row.findAll("td")
		for i in range(len(cols)):
			cvssData.append(cols[i].text.strip())
	CVSSscore.append(cvssData[0])
	scrapedData = [cvssData[i].split("\n")[0] for i in range(1, 7)]
	confidentialityImpact.append(scrapedData[0])
	integrityImpact.append(scrapedData[1])
	availibilityImpact.append(scrapedData[2])
	accessComplexity.append(scrapedData[3])
	authentication.append(scrapedData[4])
	gainedAccess.append(scrapedData[5])
	vulnType.append(cvssData[7])


def main():

	url = "https://www.cvedetails.com/vulnerability-list/vendor_id-26/product_id-32238/version_id-678559.html"
	version_id = url.split("version_id-")[1].split(".html")[0]
	print("Scraping {}".format((url)))
	soupObject = getSoupHTML(url)
	cvePagesArray = getCVEPages(soupObject)
	cveArray = []
	dates = []
	for cvePage in cvePagesArray:
		soupObject = getSoupHTML(cvePage)
		getCVEIds(soupObject, cveArray)
		getDates(soupObject, dates)

	count = 0
	for cve in cveArray:

		count += 1
		print(
			"[+] Fetching Data for CVE ID: {}. {}/{}".format(
				cve, str(count), str(len(cveArray))
			)
		)
		getCVEDetails(cve)

	writeToExcel("Windows10CVES{}".format(version_id))


if __name__ == "__main__":
	main()
